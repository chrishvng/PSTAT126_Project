---
title: "PSTAT 126 Project"
author: Jeff Shen
date: "2023-04-27"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE}
# default code chunk options
knitr::opts_chunk$set(echo = T,
                      results = 'markup',
                      message = F, 
                      warning = F,
                      fig.width = 4,
                      fig.height = 3,
                      fig.align = 'center') 

# load packages
library(faraway)
library(tidyverse)
library(tidymodels)
library(modelr)
library(ggplot2)
library(glmnet)

```
## STEP TWO

### Introduction

In this project, we will analyze the relationship between income and education using the adult.csv dataset. Rather than simply comparing income levels and education levels, we will aggregate the data by calculating the percentage of individuals with income higher then 50k for each education level. This will allow us to create a quantitative variable for the proportion of individuals in each education level who earn a high income, which we can then use to explore the relationship between education and income. The dataset of interest comes from the UCI Machine Learning Repository and is available on Kaggle as well at: https://www.kaggle.com/datasets/uciml/adult-census-income. It contains information about individuals from the 1994 Census database, including demographic variables such as age, education, marital status, occupation, and more.

The variable of interest for this analysis is the income column, and we will be using a simple linear regression model to examine its relationship with education.

### Hypotheses


Our objective is to investigate the relationship between education level and income among individuals in the 1994 Census database by aggregating the data to calculate the percentage of individuals with income higher than 50k for each education level. We aim to determine whether there is a positive linear relationship between education level and the proportion of individuals with high income.

Null hypothesis: There is no significant positive linear relationship between education and the proportion of individuals with high income in the 1994 Census database. The beta coefficient for education (β1) in a linear regression model predicting the proportion of individuals with high income is equal to zero.

$$H_0: \beta_1 = 0$$

Alternative hypothesis: There is a significant positive linear relationship between education and the proportion of individuals with high income in the 1994 Census database, with higher levels of education associated with a higher proportion of individuals with high income. The beta coefficient for education (β1) in a linear regression model predicting the proportion of individuals with high income is greater than zero.

$$H_a: \beta_1 > 0$$

We can test these hypotheses using a linear regression model and examining the p-value associated with the beta coefficient for education. If the p-value is less than our chosen significance level (typically 0.05), we can reject the null hypothesis and conclude that there is evidence of a positive linear relationship between education and the proportion of individuals with high income.

### Checking the Assumptions for Linear Regression
```{r}
income_data <- read.csv("~/Desktop/adult.csv")
head(income_data)
```

### Plots and Transformations

```{r assumption_plots, fig.show = 'show', echo=FALSE}
# Divide into education levels
edu_levels <- c("Preschool", "1st-4th", "5th-6th", "7th-8th", "9th", "10th", "11th", "12th",
                "HS-grad", "Some-college", "Assoc-acdm", "Assoc-voc", "Bachelors", "Masters", "Prof-school", "Doctorate")

# Transform each education level to an 'education number' to compare 2 quantitative variables
income_data <- income_data %>%
  mutate(education_num = match(education, edu_levels))

# Calculate the percentage of individuals making over 50k in each education group to create data aggregation for income discrepancies 
income_summary <- income_data %>%
  group_by(education_num) %>%
  summarize(prop_over_50k = mean(income == ">50K"))
prop_over_50k <- income_summary$prop_over_50k
# Create a scatterplot of education level (numerical) vs. proportion over 50k
ggplot(income_summary, aes(x = education_num, y = prop_over_50k)) +
  geom_point() +
  labs(x = "Education Level", y = "Proportion Making Over 50K", title = "Scatter Plot") +
  scale_x_continuous(breaks = c(1:length(edu_levels)), labels = edu_levels) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))

# Create a bar graph of proportion of high income individuals by education level
ggplot(data=income_summary, aes(x = education_num, y = prop_over_50k)) +
  geom_bar(stat = "identity", fill = "#377EB8") +
  labs(x = "Education Level", y = "Proportion with Income over 50K", title = "Proportion of High Income by Education") +
  scale_x_continuous(breaks = c(1:length(edu_levels)), labels = edu_levels) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
    plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))

# Create a stacked bar chart of income against education level
ggplot(data = income_data, aes(x = education_num, fill = income)) +
  geom_bar(position = "stack") +
  labs(x = "Education Level", y = "Count", title = "Distribution of Income by Education Level") +
  scale_x_continuous(breaks = 1:length(edu_levels), labels = edu_levels) +
  scale_fill_manual(values = c("#377EB8", "#E41A1C"), labels = c("<=50K", ">50K")) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))
```
Looking at the scatter plot above, it can be seen that the data roughly follows a linear path of progression and has constant variability across the plot. There is no squeeze or diversion to indicate that there is different variabilities in the graph. However, we will still perform both log and square root transformations of the graph to see if there could be a better transformation that exists for this data set. 

###Linear Regression
```{r lm_plot, fig.show='show', echo=FALSE}

# Perform linear regression
lm_fit <- lm(prop_over_50k ~ education_num, data = income_summary)
summary(lm_fit)

# Plot trend line
ggplot(income_summary, aes(x = education_num, y = prop_over_50k)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Education Level", y = "Proportion Making Over 50K") +
  scale_x_continuous(breaks = c(1:length(edu_levels)), labels = edu_levels) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
    plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))

```


Analyzing the data above, the coefficient for the variable "education_num" is estimated to be 0.046819. This suggests that for every unit increase in the "education_num" variable, the "prop_over_50k" is expected to increase by 0.046819 units. The p-value associated with the coefficient for "education_num" is 2.81e-06, which is less than 0.05. The small p-value suggests that there is strong evidence rejecting the null hypothesis (no relationship) and conclude that there is a significant relationship between education level and the proportion of individuals earning over 50k. Furthermore, The F-statistic (56.49) tests the overall significance of the linear regression model. Since the associated p-value is extremely small, we can determine that the model as a whole is statistically significant. This means that the relationship between education level and the proportion of individuals earning over 50k is not due to chance and is likely a meaningful relationship.


### Transformed Plots
```{r log_plot, fig.show='show', echo=FALSE}

# Generates summary to view R^2 value for the log transformation
lm_log_fit <- lm(prop_over_50k ~ log10(education_num), data = income_summary)
summary(lm_log_fit)

# Generates a log transformation for the scatterplot
log_plot <- ggplot(income_summary, aes(x = log10(education_num), y = prop_over_50k)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Log Education Level", y = "Proportion Making Over 50K") +
  scale_x_continuous(breaks = log10(c(1:length(edu_levels))), labels = edu_levels) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))
log_plot
```


For our initial transformation, we elected to use the log function to see if we could get a more consistent result by using the transformation. Comparing the data from linear regression and log transformation, we can see that the coefficient for the variable "education_num"increased, the F-statistic fell, and the p-value increased. The increase in the coefficient for the variable "education_num" from 0.046819 to 0.5287 suggests that the log transformation has strengthened the relationship between "education_num" and the outcome variable. The decrease in the F-statistic from 56.49 to 15.76 suggests that the log-transformed model explains less of the variation in the outcome compared to the linear regression model. This could indicate that the linear model with the original variables had a better overall fit or captured more of the relationship between the predictors and the outcome. Finally, he increase in the p-value from 2.809e-06 to 0.001397 indicates that the log-transformed model is less statistically significant compared to the linear regression model. Therefore we can conclude the original linear model is a better fit in comparison to this log transformation.


```{r sqrt_plot, fig.show='show', echo=FALSE}
# Generates summary to view R^2 value for the sqrt transformation
lm_sqrt_fit <- lm(prop_over_50k ~ sqrt(education_num), data = income_summary)
summary(lm_sqrt_fit)

# Generates a sqrt transformation for the scatterplot
sqrt_plot <- ggplot(income_summary, aes(x = sqrt(education_num), y = prop_over_50k)) +
  geom_point() + geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "Square Root of Education Level", y = "Proportion Making Over 50K") +
  scale_x_continuous(breaks = sqrt(c(1:length(edu_levels))), labels = edu_levels) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        plot.title = element_text(size = 11, hjust = 0.5, vjust = 1.5))
sqrt_plot


```

We transitioned from the log transformation to a square root transformation instead to see if we could get a more consistent result by using this transformation instead. Comparing the data from the linear regression and square root transformation, we can see that the coefficient for the variable "education_num" increased, the F-statistic fell, and the p-value increased. The increase in the coefficient for the variable "education_num" from 0.046819 to 0.22614 suggests that the log transformation has strengthened the relationship between "education_num" and the outcome variable. The decrease in the F-statistic from 56.49 to 30.05 suggests that the square root transformed model explains less of the variation in the outcome compared to the linear regression model. This once again indicate that the linear model with the original variables had the best better overall fit or captured more of the relationship between the predictors and the outcome. Finally, the increase in the p-value from 2.809e-06 to 8.085e-05 indicates that the log-transformed model is less statistically significant compared to the linear regression model. 

The values from the log and square root transformation indicated they are both less statistically significant than the original linear regression model that we had. However, comparing both of the transformations against each other, it is evident that the square root transformation is a better fit to use as a transformation.


#### Scatter Plot Analysis

Our initial scatter plot data compared education level (numerical) vs. proportion making over 50k. The graph displayed an exponential upward trend for the proportion making over 50k as the level of education increased. In order to find a better fit to the trend line, we created multiple transformations of the scatter plot in an attempt to make the plot more linear. For our transformations, we decided to manipulate the data by comparing both the log and square root of education level to the proportion making over 50k.

After analyzing the results from the new transformations, we determined that the log transformation plot created an even steeper rise in comparison to the original and as a result pushed the points even further away from the trend line. Although not as severe, the square root transformation also became steeper than the original. This meant that neither transformation linearized the data set and we concluded that the original plot showed the best fit. Furthermore, this could be confirmed by comparing the r-squared value for each of the three scatter plots. The original plot posted the highest value at 0.7872, compared to the log and square root plots at 0.4959 and 0.6595, respectively.

### Test of Beta 1 
```{r beta_coeff, echo=FALSE}
beta_1 <- coef(lm_fit)[2]
cat("beta_1 coefficient: ", beta_1, "\n")
```
Since $\beta_1 > 0$, we can reject the null hypothesis, and determine that there is in fact a positive linear relationship between education and income. 

### Confidence Interval
```{r CI_B1, echo=FALSE}

#confidence interval for education num
l.model <- lm(education_num ~ 1, income_summary)
confint(l.model, level = 0.95)

```
The 2.5% and 97.5% represent the upper and lower bounds of our 95% confidence interval. This data means that we can be 95% confident that the true population value of the intercept falls within this range.




### R^2 value and Residual Plots
Our calculated r-square value.
```{r Residuals Plots and R^2, fig.show='show', echo = FALSE}
#Assess the fit of your model. Discuss the 𝑅2 value and the residual plot(s). Remember that residual plots (not 𝑅2) determine whether a linear model is appropriate

#calculating R^2 

l.model2 <- lm(prop_over_50k ~ education_num, income_summary)

rsquare(l.model2, income_summary)
```


```{r R^2, fig.show='show', echo = FALSE}
#calculating residuals 
res <- resid(l.model2)

#plot of fitted vs residual plots
plot(fitted(l.model2), res)

#adding line to the model 
abline(0,0)

```
Plot comparison between the fitted and residual plots. The curve in the plot shows that the residuals are positive when the fitted value is small, negative when the fitted value is in the middle and positive when the fitted value is large. The fitted line doesn't describe how y behaves as x changes, since the relationship is curved.




```{r Residuals Plots for qq norm, fig.show='show', echo = FALSE}


#qq plot for residuals 
qqnorm(res)

#adding line to the plot of residuals 
qqline(res)
```

Normal QQ plot for residuals. Seems to be light tailed but overall relatively normal.

```{r Residuals Plots for density, fig.show='show', echo = FALSE}

#density plot of residuals 
plot(density(res))

```
Density plot of the residuals. Seems to be lightly right-skewed  Is the linear model appropriate or not.

### Conclusion
Overall, all three analyses indicate that education level, whether represented by linear regression, logarithmic transformation, or square root transformation, is a significant predictor of the proportion of individuals that are earning over 50k. The findings suggest that higher education levels are generally associated with a higher likelihood of individuals earning more than the 50k threshold. However, the strength and magnitude of the relationships differ slightly depending on the transformation applied to the education variable. We were surprised that although all three were significant predictors, neither our log or square root transformations gave us a more significant data than the original. Since all seem to be somewhat significant, it is likely that further analysis and interpretation may be necessary to determine which transformation will provide the best fit and most meaningful conclusions for the data set. However with 𝛽1>0, there is in fact a positive linear relationship between education and income for all of the graphs.
